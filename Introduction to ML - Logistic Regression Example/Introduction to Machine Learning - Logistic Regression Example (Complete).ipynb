{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to ML - Binary Logistic Regression Example for Beginners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is Machine Learning (ML) in a nutshell\n",
    "- “Machine learning is the science (and art) of programming computers so they can learn from data” by Aurélien Géron book (Hands-On Machine Learning with Scikit-Learn and TensorFlow)\n",
    "\n",
    "\n",
    "- ML uses statistical models and algorithms to perform tasks like predictions & classifications without explicit instructions\n",
    "\n",
    "\n",
    "- ML is a subset of Artificial Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Machine Learning Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'F:\\\\Github\\\\Python tutorials\\\\Introduction to ML - LR model\\\\Machine Learning.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6f8bb81a0873>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"F:\\\\Github\\\\Python tutorials\\\\Introduction to ML - LR model\\\\\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"Machine Learning.png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1202\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m         super(Image, self).__init__(data=data, url=url, filename=filename, \n\u001b[0;32m-> 1204\u001b[0;31m                 metadata=metadata)\n\u001b[0m\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'width'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'F:\\\\Github\\\\Python tutorials\\\\Introduction to ML - LR model\\\\Machine Learning.png'"
     ]
    }
   ],
   "source": [
    "### Graphical ML Process\n",
    "\n",
    "import os\n",
    "from IPython.display import Image\n",
    "PATH = \"F:\\\\Github\\\\Python tutorials\\\\Introduction to ML - LR model\\\\\"\n",
    "Image(filename = PATH + \"Machine Learning.png\", width=900, height=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Overview:\n",
    "\n",
    "##### Video 1:\n",
    "1. What is Machine Learning\n",
    "2. Process of Machine Learning\n",
    "3. Problem Formulation\n",
    "4. Loading the Raw Data\n",
    "5. Data Preprocessing\n",
    "    - EDA\n",
    "\n",
    "##### Video 2:\n",
    "5. Data Preprocessing\n",
    "    - Data Cleaning\n",
    "    - Feature Selection\n",
    "6. Splitting the Raw Data\n",
    "7. What is Logistic Regression Analysis\n",
    "\n",
    "##### Video 3:\n",
    "8. Running Logistic Regression\n",
    "9. Evaluating the Model\n",
    "10. Hyper Parameter Tuning\n",
    "11. Final Model with Selected Parameters\n",
    "12. How to use our L. Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Problem Formulation\n",
    "\n",
    "- In this example, we want to investigate \"what factors/variables affect a good or a bad loan\"\n",
    "- Make predictions whether a customer should get its loan approved or not based on their characteristics\n",
    "- Hence our Dependent variable (y) is the Loan Approval (Yes/No) or if the Loan is Good or Bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing / Installing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages / libraries\n",
    "import os #provides functions for interacting with the operating system\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, explained_variance_score, confusion_matrix, accuracy_score, classification_report, log_loss\n",
    "from math import sqrt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# To install sklearn type \"pip install numpy scipy scikit-learn\" to the anaconda terminal\n",
    "\n",
    "# To change scientific numbers to float\n",
    "np.set_printoptions(formatter={'float_kind':'{:f}'.format})\n",
    "\n",
    "# Increases the size of sns plots\n",
    "sns.set(rc={'figure.figsize':(12,10)})\n",
    "\n",
    "# import sys\n",
    "# !conda list Check the packages installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Loading the Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "raw_data = pd.read_csv('F:\\\\Github\\\\Python tutorials\\\\Introduction to ML - Logistic Regression\\\\Logistic Regression Dummy Data v3.csv')\n",
    "\n",
    "# print the shape\n",
    "print(raw_data.shape)\n",
    "\n",
    "#runs the first 5 rows\n",
    "raw_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for null values\n",
    "\n",
    "raw_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the NULL observations\n",
    "\n",
    "raw_data[raw_data['Employment History'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the NULL values\n",
    "raw_data = raw_data.dropna(subset = ['Employment History'])\n",
    "\n",
    "# Printing the shape\n",
    "print(raw_data.shape)\n",
    "\n",
    "# Visualize the NULL observations\n",
    "raw_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate all the elements whithin each Feature \n",
    "\n",
    "for column in raw_data:\n",
    "    unique_values = np.unique(raw_data[column])\n",
    "    nr_values = len(unique_values)\n",
    "    if nr_values <= 10:\n",
    "        print(\"The number of values for feature {} is: {} -- {}\".format(column, nr_values, unique_values))\n",
    "    else:\n",
    "        print(\"The number of values for feature {} is: {}\".format(column, nr_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data using seaborn Pairplots\n",
    "\n",
    "g = sns.pairplot(raw_data)\n",
    "\n",
    "# Notes: Do not run this on a big dataset. Filter the columns first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the outlier\n",
    "\n",
    "raw_data = raw_data[raw_data['Age'] < 100]\n",
    "\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data using seaborn Pairplots\n",
    "\n",
    "g = sns.pairplot(raw_data, hue = 'Good Loan')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating the distr of y\n",
    "\n",
    "sns.countplot(x = 'Good Loan', data = raw_data, palette = 'Set3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Looping through all the features by our y variable - see if there is relationship\n",
    "\n",
    "features = ['Type of Account', 'Account History', 'Reason for the Loan',\n",
    "       'Account Savings', 'Employment History',\n",
    "       'Individual Stauts', 'Other Loans', 'Security / Collateral',\n",
    "       'Residence Status', 'Job', 'Completed Other loan?']\n",
    "\n",
    "for f in features:\n",
    "    sns.countplot(x = f, data = raw_data, palette = 'Set3', hue = 'Good Loan')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making categorical variables into numeric representation\n",
    "\n",
    "new_raw_data = pd.get_dummies(raw_data, columns = features)\n",
    "\n",
    "# Notes:\n",
    "# We can also do this with Label Encoding and OneHotEncoder from the preprocessing library\n",
    "\n",
    "print(raw_data.shape)\n",
    "# print the shape\n",
    "print(new_raw_data.shape)\n",
    "\n",
    "# Creating a new 0-1 y variable\n",
    "#new_raw_data['Loan Approved2'] = 0\n",
    "new_raw_data['Good Loan'][new_raw_data['Good Loan'] == 'Yes'] = 1\n",
    "new_raw_data['Good Loan'][new_raw_data['Good Loan'] == 'No'] = 0\n",
    "\n",
    "# Visualizing the data\n",
    "new_raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "\n",
    "###### We do not need to normalize / standardize the data in Logistic Regression due to the logistic function (0 or 1)\n",
    "###### Once a value crosses the decision boundary (0.5 threshold), it saturates\n",
    "###### After the 0.5 or before, there is no additional value to be added from smaller or larger values\n",
    "###### more details: https://medium.com/@swethalakshmanan14/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Feature Selection\n",
    "In this example, we do not have many variables so we might use all of the data but in some cases, you have thousands of variables and you will need to filter them in order to save computational time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps of Running Feature Importance\n",
    "- Split the data into X & y\n",
    "- Run a Tree-based estimators (i.e. decision trees & random forests) \n",
    "- Run Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X & y\n",
    "\n",
    "X = new_raw_data.drop('Good Loan', axis = 1).values\n",
    "y = new_raw_data['Good Loan']\n",
    "\n",
    "y = y.astype(int)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a Tree-based estimators (i.e. decision trees & random forests)\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=15, criterion = 'entropy', max_depth = 10)\n",
    "dt.fit(X,y)\n",
    "\n",
    "# If you want to learn how Decesion Trees work, read here: https://www.datacamp.com/community/tutorials/decision-tree-classification-python\n",
    "# Official Doc: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "- The importance of a feature is calculated as the (normalized) total reduction of entropy (other criterions too) brought by that feature or the higher information gain\n",
    "- To understand the maths, read this: https://towardsdatascience.com/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Feature Importance\n",
    "\n",
    "fi_col = []\n",
    "fi = []\n",
    "\n",
    "for i,column in enumerate(new_raw_data.drop('Good Loan', axis = 1)):\n",
    "    print('The feature importance for {} is : {}'.format(column, dt.feature_importances_[i]))\n",
    "    \n",
    "    fi_col.append(column)\n",
    "    fi.append(dt.feature_importances_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Dataframe\n",
    "fi_col\n",
    "fi\n",
    "\n",
    "fi_df = zip(fi_col, fi)\n",
    "fi_df = pd.DataFrame(fi_df, columns = ['Feature','Feature Importance'])\n",
    "fi_df\n",
    "\n",
    "\n",
    "# Ordering the data\n",
    "fi_df = fi_df.sort_values('Feature Importance', ascending = False).reset_index()\n",
    "\n",
    "# Creating columns to keep\n",
    "columns_to_keep = fi_df['Feature'][0:40]\n",
    "\n",
    "fi_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "- Please note that we have not normalised / scale our data\n",
    "- Please note that we have not done any feature engineering - created new features\n",
    "- Please note that we have not joined multiple datasets together\n",
    "- Please note that we have not aggregated any of our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Splitting the Raw Data - Hold-out validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes\n",
    "\n",
    "print(new_raw_data.shape)\n",
    "print(new_raw_data[columns_to_keep].shape)\n",
    "\n",
    "# new_raw_data = new_raw_data[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X & y\n",
    "\n",
    "X = new_raw_data[columns_to_keep].values\n",
    "X\n",
    "\n",
    "y = new_raw_data['Good Loan']\n",
    "y = y.astype(int)\n",
    "y\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold-out validation\n",
    "\n",
    "# first one\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, test_size=0.2, random_state=15)\n",
    "\n",
    "# Second one\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, train_size = 0.9, test_size=0.1, random_state=15)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_valid.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(y_valid.shape)\n",
    "\n",
    "# Official Doc: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating the distr of all ys\n",
    "\n",
    "ax = sns.countplot(x = y_valid, palette = \"Set3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. What is Logistic Regression \n",
    "\n",
    "- Famous statistical method for predicting two or more binary classes; not continues numbers. \n",
    "- Hence, Logistic regression is used for classification problems\n",
    "- To make it work, we transform our linear regression line into a logistic regression curve so we can get a good fit of our data (see pics below) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graphical\n",
    "\n",
    "PATH = \"F:\\\\Github\\\\Python tutorials\\\\Introduction to ML - Logistic Regression\\\\\"\n",
    "Image(filename = PATH + \"Logistic1.png\", width=900, height=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works:\n",
    " \n",
    "- We fit an \"S\" Shape logistic Function\n",
    "- The curve tells you the porbability if a loan is good or bad\n",
    "- If we have a high Credit score, there is a high probability that it's a good loan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graphical\n",
    "\n",
    "PATH = \"F:\\\\Github\\\\Python tutorials\\\\Introduction to ML - Logistic Regression\\\\\"\n",
    "Image(filename = PATH + \"Logistic2.png\", width=900, height=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How Maximum likelihood works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graphical\n",
    "\n",
    "PATH = \"F:\\\\Github\\\\Python tutorials\\\\Introduction to ML - Logistic Regression\\\\\"\n",
    "Image(filename = PATH + \"Logistic3.png\", width=900, height=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link for Discrete Variables: https://youtu.be/vN5cNN2-HWE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Running Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d693c8d5e958>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training my model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlog_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlog_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "# Training my model\n",
    "\n",
    "log_reg = LogisticRegression(random_state=10, solver = 'lbfgs')\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# SKLearn doc: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods we can use in Logistic\n",
    "\n",
    "# predict - Predict class labels for samples in X\n",
    "log_reg.predict(X_train)\n",
    "y_pred = log_reg.predict(X_train)\n",
    "\n",
    "# predict_proba - Probability estimates\n",
    "pred_proba = log_reg.predict_proba(X_train)\n",
    "\n",
    "# coef_ - Coefficient of the features in the decision function\n",
    "log_reg.coef_\n",
    "\n",
    "# score- Returns the mean accuracy on the given test data and labels - below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy on Train\n",
    "print(\"The Training Accuracy is: \", log_reg.score(X_train, y_train))\n",
    "\n",
    "# Accuracy on Test\n",
    "print(\"The Testing Accuracy is: \", log_reg.score(X_test, y_test))\n",
    "\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_train, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix function\n",
    "\n",
    "def plot_confusion_matrix(cm, classes=None, title='Confusion matrix'):\n",
    "    \"\"\"Plots a confusion matrix.\"\"\"\n",
    "    if classes is not None:\n",
    "        sns.heatmap(cm, cmap=\"YlGnBu\", xticklabels=classes, yticklabels=classes, vmin=0., vmax=1., annot=True, annot_kws={'size':50})\n",
    "    else:\n",
    "        sns.heatmap(cm, vmin=0., vmax=1.)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing cm\n",
    "\n",
    "cm = confusion_matrix(y_train, y_pred)\n",
    "cm_norm = cm / cm.sum(axis=1).reshape(-1,1)\n",
    "\n",
    "plot_confusion_matrix(cm_norm, classes = log_reg.classes_, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.sum(axis=1)\n",
    "cm_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diag(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating False Positives (FP), False Negatives (FN), True Positives (TP) & True Negatives (TN)\n",
    "\n",
    "FP = cm.sum(axis=0) - np.diag(cm)\n",
    "FN = cm.sum(axis=1) - np.diag(cm)\n",
    "TP = np.diag(cm)\n",
    "TN = cm.sum() - (FP + FN + TP)\n",
    "\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP / (TP + FN)\n",
    "print(\"The True Positive Rate is:\", TPR)\n",
    "\n",
    "# Precision or positive predictive value\n",
    "PPV = TP / (TP + FP)\n",
    "print(\"The Precision is:\", PPV)\n",
    "\n",
    "# False positive rate or False alarm rate\n",
    "FPR = FP / (FP + TN)\n",
    "print(\"The False positive rate is:\", FPR)\n",
    "\n",
    "\n",
    "# False negative rate or Miss Rate\n",
    "FNR = FN / (FN + TP)\n",
    "print(\"The False Negative Rate is: \", FNR)\n",
    "\n",
    "\n",
    "\n",
    "##Total averages :\n",
    "print(\"\")\n",
    "print(\"The average TPR is:\", TPR.sum()/2)\n",
    "print(\"The average Precision is:\", PPV.sum()/2)\n",
    "print(\"The average False positive rate is:\", FPR.sum()/2)\n",
    "print(\"The average False Negative Rate is:\", FNR.sum()/2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logarithmic loss - or Log Loss - or cross-entropy loss\n",
    "\n",
    "- Log Loss is an error metric\n",
    "- This is the loss function used in (multinomial) logistic regression and extensions of it such as neural networks, defined as the negative log-likelihood of the true labels given a probabilistic classifier’s predictions. \n",
    "\n",
    "- Why it's important? For example, imagine having 2 models / classifiers that both predict one observation correctly (Good Loan). However, 1 classifier has a predicted probability of 0.54 and the other 0.95. Which one will you choose? Classification Accuracy will not help here as it will get both on 100%\n",
    "\n",
    "- Doc: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Log loss on training\n",
    "print(\"The Log Loss on Training is: \", log_loss(y_train, pred_proba))\n",
    "\n",
    "# Running Log loss on testing\n",
    "pred_proba_t = log_reg.predict_proba(X_test)\n",
    "print(\"The Log Loss on Testing Dataset is: \", log_loss(y_test, pred_proba_t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Hyper Parameter Tuning\n",
    "\n",
    "- We will loop over parameter C (Inverse of regularization strength).\n",
    "- Inverse of regularization strength helps to avoid overfitting - it penalizes large values of your parameters\n",
    "- It also helps to find Global Minimum by moving to better \"solutions\" from local minimum to global minimum\n",
    "- The values of C to search should be n-equally-spaced values in log space ranging from 1e-5 to 1e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.geomspace(1e-5, 1e5, num=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a range for C values\n",
    "np.geomspace(1e-5, 1e5, num=20)\n",
    "\n",
    "# ploting it\n",
    "plt.plot(np.geomspace(1e-5, 1e5, num=20)) #  uniformly distributed in log space\n",
    "plt.plot(np.linspace(1e-5, 1e5, num=20)) # uniformly distributed in linear space, instead of log space\n",
    "# plt.plot(np.logspace(np.log10(1e-5) , np.log10(1e5) , num=20)) # same as geomspace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping over the parameters\n",
    "\n",
    "C_List = np.geomspace(1e-5, 1e5, num=20)\n",
    "CA = []\n",
    "Logarithmic_Loss = []\n",
    "\n",
    "for c in C_List:\n",
    "    log_reg2 = LogisticRegression(random_state=10, solver = 'lbfgs', C=c)\n",
    "    log_reg2.fit(X_train, y_train)\n",
    "    score = log_reg2.score(X_test, y_test)\n",
    "    CA.append(score)\n",
    "    print(\"The CA of C parameter {} is {}:\".format(c, score))\n",
    "    pred_proba_t = log_reg2.predict_proba(X_test)\n",
    "    log_loss2 = log_loss(y_test, pred_proba_t)\n",
    "    Logarithmic_Loss.append(log_loss2)\n",
    "    print(\"The Logg Loss of C parameter {} is {}:\".format(c, log_loss2))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting the outcomes in a Table\n",
    "\n",
    "# reshaping\n",
    "CA2 = np.array(CA).reshape(20,)\n",
    "Logarithmic_Loss2 = np.array(Logarithmic_Loss).reshape(20,)\n",
    "\n",
    "# zip\n",
    "outcomes = zip(C_List, CA2, Logarithmic_Loss2)\n",
    "\n",
    "#df\n",
    "df_outcomes = pd.DataFrame(outcomes, columns = [\"C_List\", 'CA2','Logarithmic_Loss2'])\n",
    "\n",
    "#print\n",
    "df_outcomes\n",
    "\n",
    "# Ordering the data (sort_values)\n",
    "df_outcomes.sort_values(\"Logarithmic_Loss2\", ascending = True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way of doing the above\n",
    "# Scikit-learn offers a LogisticRegressionCV module which implements Logistic Regression \n",
    "# with builtin cross-validation to find out the optimal C parameter\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3, random_state=0, shuffle=True)\n",
    "\n",
    "# Logistic Reg CV\n",
    "Log_reg3 = LogisticRegressionCV(random_state=15, Cs = C_List, solver ='lbfgs')\n",
    "Log_reg3.fit(X_train, y_train)\n",
    "print(\"The CA is:\", Log_reg3.score(X_test, y_test))\n",
    "pred_proba_t = Log_reg3.predict_proba(X_test)\n",
    "log_loss3 = log_loss(y_test, pred_proba_t)\n",
    "print(\"The Logistic Loss is: \", log_loss3)\n",
    "\n",
    "print(\"The optimal C parameter is: \", Log_reg3.C_)\n",
    "\n",
    "\n",
    "\n",
    "# Doc: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross validation:\n",
    "Advantage: K-fold cross validation uses all the training data to train the model, by applying k different splits; repeated train-test splits converge to the true accuracy given that the training data is representable for the underlying distribution; however in practise this is often overoptimistic.\n",
    "Disadvantage: The disadvantage of this method is that the training algorithm has to be rerun from the beginning k times, which means it takes k times as much computation to get an evaluation. Additionally, if you want to test the performance on a completely new dataset that the algorithm has never seen, you cannot do this with k-fold cross validation.\n",
    "\n",
    "### Hold-out:\n",
    "Advantage: The advantage of Hold-out is that you can test how your model performs on completely unseen data that you haven't used when training the model. Additionally, Hold-out is usually much faster and less computationally expensive. \n",
    "Disadvantage: The evaluation may depend heavily on which data points end up in the training set and which end up in the test set, and thus the evaluation may be significantly different depending on how the division is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Maybe we have a different metric we want to track\n",
    "\n",
    "# Looping over the parameters\n",
    "\n",
    "C_List = np.geomspace(1e-5, 1e5, num=20)\n",
    "CA = []\n",
    "Logarithmic_Loss = []\n",
    "\n",
    "for c in C_List:\n",
    "    log_reg2 = LogisticRegression(random_state=10, solver = 'lbfgs', C=c)\n",
    "    log_reg2.fit(X_train, y_train)\n",
    "    score = log_reg2.score(X_test, y_test)\n",
    "    CA.append(score)\n",
    "    print(\"The CA of C parameter {} is {}:\".format(c, score))\n",
    "    pred_proba_t = log_reg2.predict_proba(X_test)\n",
    "    log_loss2 = log_loss(y_test, pred_proba_t)\n",
    "    Logarithmic_Loss.append(log_loss2)\n",
    "    print(\"The Logg Loss of C parameter {} is {}:\".format(c, log_loss2))\n",
    "    print(\"\")\n",
    "    \n",
    "    y_pred = log_reg2.predict(X_train)\n",
    "    cm = confusion_matrix(y_train, y_pred)\n",
    "    cm_norm = cm / cm.sum(axis=1).reshape(-1,1)\n",
    "    plot_confusion_matrix(cm_norm, classes = log_reg.classes_, title='Confusion matrix')\n",
    "    plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a Dummy Classifier\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "score = dummy_clf.score(X_test, y_test)\n",
    "\n",
    "pred_proba_t = dummy_clf.predict_proba(X_test)\n",
    "log_loss2 = log_loss(y_test, pred_proba_t)\n",
    "\n",
    "print(\"Testing Acc:\", score)\n",
    "print(\"Log Loss:\", log_loss2)\n",
    "\n",
    "\n",
    "# Doc: https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Final Model with Selected Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Model \n",
    "\n",
    "log_reg3 = LogisticRegression(random_state=10, solver = 'lbfgs', C=784.759970)\n",
    "log_reg3.fit(X_train, y_train)\n",
    "score = log_reg3.score(X_valid, y_valid)\n",
    "\n",
    "pred_proba_t = log_reg3.predict_proba(X_valid)\n",
    "log_loss2 = log_loss(y_valid, pred_proba_t)\n",
    "\n",
    "print(\"Testing Acc:\", score)\n",
    "print(\"Log Loss:\", log_loss2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. How to use our L. Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "\n",
    "- Option 1: Deploy the model in a CRM System or the Cloud or Viz tools and automaticaly decide if a customer should get his loan approved or not\n",
    "\n",
    "\n",
    "- Option 2: Analyse the factors that affect a good/bad loan and help the business understand this. Then the business can educate their clients what they need to improve in order to get a loan\n",
    "\n",
    "\n",
    "- Option 3: Deploy this model in an open bank website where customers can automatically see if their loan will get approved or not; saving time & cost for the bank/business\n",
    "\n",
    "\n",
    "- Option 4 - What do you think?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
